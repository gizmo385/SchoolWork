\documentclass{article}%
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}%
\usepackage[a4paper,includeheadfoot,margin=3cm]{geometry}
\setcounter{MaxMatrixCols}{30}
%TCIDATA{OutputFilter=late$x2$.dll}
%TCIDATA{Version=5.00.0.2552}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{Created=Thursday, August 21, 2008 14:03:59}
%TCIDATA{LastRevised=Wednesday, October 01, 2014 12:46:33}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{Language=American English}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\usepackage{fancyhdr}
\setlength\headheight{26pt}
\pagestyle{fancy}
\lhead{{\footnotesize CSc 483 - Assignment 3}}
\rhead{{\footnotesize Christopher Chapline}}
\begin{document}
\section*{Problem 1}
\subsection*{Part 1}
The results of the query "information retrieval" are: Document #1(0.509492), Document #2(0.509492), Document #3(0.095837)
\subsection*{Part 2}
The default model for scoring documents in Lucene is the tf-idf scoring model based on the following formula:

$$\text{cosine-similarity}(query, document) = \frac{V(query) \cdot V(document)}{|V(query)| \cdot |V(document)|}$$

\noindent where $V(query)$ and $V(document)$ denote the weighted query and document vectors respectively. This is complicated by the addition of things such as "query boosting" whereby a user can boost the importance of a term in a Lucene query with the "\string^" operator.  \\
\\
In addition to using the tf-idf model, the boolean query model is used before scoring occurs to limit the number of documents that must be scored by the system and to handle boolean logical operators that are found in the query such as $AND$, $OR$, and $NOT$.
\subsection*{Part 3}
\subsubsection*{Part a}
The results of the query "information AND retrieval" are: Document #1(0.509492), Document #2(0.509492)
\subsubsection*{Part b}
The results of the query "information AND NOT retrieval" is: Document #3(0.312500)
\subsubsection*{Part c}
The results of the query "information AND retrieval WITHIN 1 WORD OF EACH OTHER" is: Document #1(0.714901)

\section*{Problem 2}
First we should convert the entries that we can to gaps. This yields the list: 777, 16966, 276325, 30975268. Next, we should convert these to binary:\\
\begin{center}
    777 \rightarrow \text{00000011 00001001} \\
    16966 \rightarrow \text{01000010 01000110} \\
    276325 \rightarrow \text{00000100 00110111 01100101} \\
    30975268 \rightarrow \text{00000001 11011000 10100101 00100100} \\
\end{center}

\subsection*{Variable Byte Encoding}
We will dedicate the first bit (leftmost) of each 8-bit block to be the continuation bit. Thus, we can encode the numbers like so:\\
00000110 10000100, 00000010  00000100 11000110, 00010000  01101110 11100101, 00000111  00110001 00100101 1100100

\subsection*{Gamma Codes}

777 = 1111111110100001001\\
16966 = 11111111111111000001001000110\\
276325 = 1111111111111111110000011011101100101\\
30972568 = 1111111111111111111111110110110001010010100100100\\
\\
So the final encoding is: 11111111 10100001 00111111 11111111 10000010 01000110 11111111 11111111 11000001 10111011 00101111 11111111 11111111 11111011 01100010 10010100 100100

\section*{Problem 3}

Parsed Gamma coding: 1001 110 11 1110111 11 1.\\
Gaps: 9, 6, 3, 119, 3, 1\\
Doc Ids: 9, 15, 18, 137, 140, 141

\section*{Problem 4}

\begin{tabular}{| l | l | l | l | l |}
    \hline
                & Doc1  & Doc2  & Doc3  \\ \hline
    car         & 4.01  & 2.64  & 2.93  \\ \hline
    auto        & 3.07  & 5.24  & 2.08  \\ \hline
    insurance   & 1.62  & 4.08  & 3.99  \\ \hline
    best        & 3.22  & 1.50  & 3.35  \\ \hline
\end{tabular}

\section*{Problem 5}

\subsection*{Part 1}

First, consider the formula for $idf_t$, where $t$ is some term is defined as:

$$idf_t = log_{10}\left(\frac{N}{df_t}\right)$$

\noindent where $df_t$ is the document frequency of $t$ and $N$ is the number of documents in the collection.\\
\\
For a term $t$ that occurs in every document, its document frequency would be equal to $N$. In this case, the fraction $\frac{N}{df_t}$ simply reduces to 1 and the $idf_t$ is the same as $log_{10}(1)$, which is just 0. This would make its tf-idf weight 0, meaning it would have no effect on the ranking of different documents in the collection. This mimics the behavior of a stop word list, which seeks to ignore certain words that add nothing to a documents usefulness (the, is, etc.).\\
\\
While these are functionally equivalent as far as queries are concerned, handling useless words at indexing time would likely prove to be more efficient in the long run as it would reduce the number of weights that you need to compute for queries.

\subsection*{Part 2}

Rewriting $Score(q, d)$ in terms of the $idf$ definition:

$$
\sum_{t \in q}\left(tf \cdot log_{b}\left(\frac{N}{df_t}\right)\right)
$$
\noindent where $b$ is usually considered to be 10. We can rewrite this formula in terms of logarithms with base 10:

$$
\sum_{t \in q}
\left(
tf
\cdot
\frac{log_{10}\left(\frac{N}{df_t}\right)}{log_{10}(b)}
\right)
$$

\noindent Notice that the logarithms that involve $b$ are all in the denominators of their respective functions. Since logarithm is a monotonically increasing function, we know that as $b$ gets larger, the result of $log_{10}(b)$ will get larger. This means that the score weights will all decrease. While all score weights will decrease, they will still retain the same relative ordering and the results of queries will be the same.
\end{document}
