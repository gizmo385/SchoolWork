\documentclass{article}%
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{hyperref}%
\usepackage[a4paper,includeheadfoot,margin=0.5in]{geometry}
\setcounter{MaxMatrixCols}{30}
%TCIDATA{OutputFilter=late$x2$.dll}
%TCIDATA{Version=5.00.0.2552}
%TCIDATA{CSTFile=40 LaTeX article.cst}
%TCIDATA{Created=Thursday, August 21, 2008 14:03:59}
%TCIDATA{LastRevised=Wednesday, October 01, 2014 12:46:33}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Blank - Standard LaTeX Article">}
%TCIDATA{Language=American English}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\usepackage{fancyhdr}
\setlength\headheight{26pt}
\pagestyle{fancy}
\lhead{{\footnotesize Assignment 4}}
\rhead{{\footnotesize Christopher Chapline}}
\begin{document}

\section*{Problem 1}
We must first create contingency tables for each of the terms in our query:\\
\\
Contingency table for "obama": \\
\begin{tabular}{| l | l | l | l |}
    \hline
    Documents                   &  Relevant     & Nonrelevant   & Total         \\ \hline
    "obama" Present $x_t = 1$   &  $s = 1.5$    & 2.5           & $df_t = 4$    \\ \hline
    "obama" Absent $x_t = 0$    &  0.5          & 0.5           & 1           \\ \hline
    Total                       &  $S = 2$    & 3               & $N = 5$       \\ \hline
\end{tabular}\\
\\
$p_t = \frac{s}{S} = \frac{\frac{3}{2}}{2} = \frac{3}{4}$ \hfill
$u_t = \frac{df_t - s}{N - S} = \frac{4 - \frac{3}{2}}{5 - 2} = \frac{5}{6}$
\vspace{2mm}\\
$c_t = log \frac{p_t}{1- p_t} - log \frac{u_t}{1 - u_t}
= log \left(\frac{\frac{3}{4}}{1 - \frac{3}{4}}\right) - log \left(\frac{\frac{5}{6}}{1 - \frac{5}{6}}\right)
= log(3) - log(5) \approx -0.222$
\vspace{5mm}\\
Contingency table for "health": \\
\begin{tabular}{| l | l | l | l |}
    \hline
    Documents                   &  Relevant     & Nonrelevant   & Total         \\ \hline
    "health" Present $x_t = 1$  &  $s = 1.5$    & 1.5           & $df_t = 3$    \\ \hline
    "health" Absent $x_t = 0$   &  0.5          & 1.5           & 2             \\ \hline
    Total                       &  $S = 2$      & 3             & $N = 5$       \\ \hline
\end{tabular}\\
\\
$p_t = \frac{s}{S} = \frac{\frac{3}{2}}{2} = \frac{3}{4}$ \hfill
$u_t = \frac{df_t - s}{N - S} = \frac{3 - \frac{3}{2}}{5 - 2} = \frac{1}{2}$
\vspace{2mm}\\
$c_t = log \frac{p_t}{1- p_t} - log \frac{u_t}{1 - u_t}
= log \left(\frac{\frac{3}{4}}{1 - \frac{3}{4}}\right) - log \left(\frac{\frac{1}{2}}{1 - \frac{1}{2}}\right)
= log(3) - log(1) \approx 0.477$
\vspace{5mm}\\
Contingency table for "plan": \\
\begin{tabular}{| l | l | l | l |}
    \hline
    Documents                   &  Relevant     & Nonrelevant   & Total         \\ \hline
    "plan" Present $x_t = 1$    &  $s = 1.5$    & 1.5           & $df_t = 3$    \\ \hline
    "plan" Absent $x_t = 0$     &  0.5          & 1.5           & 2             \\ \hline
    Total                       &  $S = 2$      & 3             & $N = 5$       \\ \hline
\end{tabular}\\
$p_t = \frac{s}{S} = \frac{\frac{3}{2}}{2} = \frac{3}{4}$ \hfill
$u_t = \frac{df_t - s}{N - S} = \frac{3 - \frac{3}{2}}{5 - 2} = \frac{1}{2}$
\vspace{2mm}\\
$c_t = log \frac{p_t}{1- p_t} - log \frac{u_t}{1 - u_t}
= log \frac{\frac{3}{4}}{1- \frac{3}{4}} - log \frac{\frac{1}{2}}{1 - \frac{1}{2}}
= log(3) - log(1) \approx 0.477$
\vspace{5mm}\\
Now that we have calculated the $c_t$ value for each of the terms in our query, we can now calculate the RSV value for each of the documents:\\
\\
Doc1: $\sum_{x_t=q_t=1}c_t = -0.222 + 0.477 = 0.255$\\
\\
Doc2: $\sum_{x_t=q_t=1}c_t = -0.222 + 0.477 = 0.255$\\
\\
Doc3: $\sum_{x_t=q_t=1}c_t = -0.222 + 0.477 + 0.477 = 0.732$\\
\section*{Problem 2}
Consider the following set of words and their English classification:\\
\begin{tabular}{| l | l | l | l |}
    \hline
    event   & word  & English?  & probability \\ \hline
    1       & ozb   & no        & $\frac{4}{9}$ \\ \hline
    2       & uzu   & no        & $\frac{4}{9}$ \\ \hline
    3       & zoo   & yes       & $\frac{1}{18}$ \\ \hline
    4       & bun   & yes       & $\frac{1}{18}$ \\ \hline
\end{tabular}
\subsection*{Part 1}
The priors are: $P(\text{English}) = \frac{1}{9}$ and $P(\neg{\text{English}}) = \frac{8}{9}$.
Calculating probabilities for individual letters leads us to the following table:\\
\\
\begin{tabular}{| l | l | l |}
    \hline
        & $P(t | \text{English})$   & $P(t | \neg{\text{English}})$ \\ \hline
    b   & $0.17$             & $0.17$ \\ \hline
    n   & $0.17$             & $0.01$ \\ \hline
    o   & $0.33$             & $0.17$ \\ \hline
    u   & $0.17$             & $0.33$ \\ \hline
    z   & $0.17$             & $0.33$ \\ \hline
\end{tabular}
\subsection*{Part 2}
Now that we have our priors and probabilities, we can determine whether or not "zoo" would be classified as English. There are
two probabilities that we must consider:

$P(\text{English} | \text{zoo}) \propto
P(\text{z} | \text{English}) \cdot P(\text{o} | \text{English})^2 \cdot P(\text{English})
\approx 0.17 \cdot 0.33^2 \cdot 0.11
= 0.002
$

$P(\neg\text{English} | \text{zoo}) \propto
P(\text{z} | \neg\text{English}) \cdot P(\text{o} | \neg\text{English})^2 \cdot P(\neg\text{English})
\approx 0.33 \cdot 0.17^2 \cdot 0.89
= 0.008$\\
\\
Since the probability $P(\neg\text{English} | \text{zoo})$ is higher than that of $P(\text{English} | \text{zoo})$, we can classify
zoo as being non-English. It is important to understand why this model does not classify the word "zoo" as English despite "zoo" being
one of the training inputs. First of all, we are computing probabilities in this model using a positional indepdence assumption on letters. This
means that just because a word is in the training set does not mean it will be classified the same in the model. Furthermore, there are more
training inputs that are NOT English, meaning that the probability of encountering English in this model is lower.
\section*{Problem 3}
Consider the following series of documents:\\
\\
\begin{tabular}{l | l}
    docID & Document Text \\ \hline
    1 & click go the shears boys click click click \\ \hline
    2 & click click \\ \hline
    3 & metal here \\ \hline
    4 & metal shears click here \\ \hline
\end{tabular}\\
\vspace{5mm}\\
Computing the probability for a query based upon a document will be done using the following formula:

$P(q | d) = \prod \left( \lambda P(t | M_d) + (1 - \lambda) P(t | M_c)\right)$\\
\\
In this formula, $\lambda = 0.5$, $M_d$ is the language model for the document and $M_c$ is the language model for the entire collection.
To compute these, we need to generate language models across our collection for the terms in each query. Additionally, for each query, we'll
need to create language models for each document in the collection.\\
\\
First we will compute the $M_c$ values for each of the terms in the three queries.\\
\begin{tabular}{| l | l |}
    \hline
    query term  & $M_c$ \\ \hline
    click       & $P(\text{click} | c) =  0.44$ \\ \hline
    shears      & $P(\text{shears} | c) =  0.13$ \\ \hline
\end{tabular}\\
\\
Now we will compute the $M_d$ values for each document for each of the terms in the three queries:\\
\begin{tabular}{| l | l | l | l | l | l |}
    \hline
    query term  & $M_{Doc1}$    & $M_{Doc2}$    & $M_{Doc3}$    & $M_{Doc4}$ \\ \hline
    click       & 0.5           & 1             & 0.00          & 0.25 \\ \hline
    shears      & 0.13          & 0.00          & 0.00          & 0.25 \\ \hline
\end{tabular}\\
\\
Here are the probabilities for three queries in this language model broken into two tables:\\
\\
\begin{tabular}{| l | l | l | l |}
    \hline
    Query           & $\lambda M_c$ & Doc1                                  & Doc2 \\ \hline
    click           &  $0.22$       & $0.22 + 0.50(0.50) = 0.47$            & $0.22 + 0.50(1.00) = 0.72$ \\ \hline
    shears          &  $0.07$       & $0.07 + 0.50(0.13) = 0.14$            & $0.07 + 0.50(0.00) = 0.07$ \\ \hline
    click shears    &  $0.03$       & $0.03 + 0.50(0.50 \cdot 0.13) = .06$  & $0.03 + 0.50(1.00 * 0.00) = 0.03$ \\ \hline
\end{tabular}\\
\vspace{2mm}
\\
\begin{tabular}{| l | l | l | l |}
    \hline
    Query           & $\lambda M_c$ & Doc3                                  & Doc4 \\ \hline
    click           &  $0.22$       & $0.22 + 0.50(0.00) = 0.22$            & $0.22 + 0.50(0.25) = 0.35$ \\ \hline
    shears          &  $0.07$       & $0.07 + 0.50(0.00) = 0.07$            & $0.07 + 0.50(0.25) = 0.20$ \\ \hline
    click shears    &  $0.03$       & $0.03 + 0.50(0.00 \cdot 0.00) = .03$  & $0.03 + 0.50(0.25 * 0.025) = 0.06$ \\ \hline
\end{tabular}\\
\\
Using these values as $P(q | d)$ values, we can rank the documents for each query:
\begin{itemize}
    \item "click" - Doc2, Doc1, Doc4, Doc3
    \item "shears" - Doc4, Doc1, Doc2, Doc3
    \item "click shears" - Doc1, Doc4, Doc2, Doc3
\end{itemize}
\section*{Problem 4}
\subsection*{Part 3}
For the query "information retrieval", the scores with and without smoothing for the documents from Assignment 3 are:\\
\\
\begin{tabular}{| l | l | l | l | l |}
    \hline
                & Document #1   & Document #2   & Document #3   & Document #4 \\ \hline
    Smoothed    & 0.008264      & 0.003906      & 0.0           & N/A \\ \hline
    Unsmoothed  & 0.005245      & 0.003384      & 0.001615      & N/A \\ \hline
\end{tabular}\\
\vspace{3mm}\\
The smoothing used for this implementation was the linear interpolation model.
\end{document}
